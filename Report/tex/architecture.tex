\BigLetter{D}{efining} and designing a distributed system used as a file archive is a compromising process of electing architecture, communication and security among others. This chapter describes and discusses the choices and opportunities regarding what henceforward will be denoted as \CodeNameFull. 

\begin{figure}
	\centering
	\includegraphics[scale=0.60]{pdf/sofa-overview.pdf}
	\caption[General overview of \CodeName]{General overview of \CodeName and the participating components and their associates, including $n_g$ gateways denoted by $\text{g}1 \ldots \text{g}n_g$, $n_s$ storage nodes by $\text{s}1 \ldots \text{s}n_s$ and $n_m$ monitors by $\text{m}1 \ldots \text{m}n_m$. \label{fig:sofa-overview}}
\end{figure}

\section{Overview}
This project is an alternative to the existing file archives targeting big data analysis frameworks; the architectural structure will be influenced and inspired by those described in related work (Section \ref{sec:related}), both by characteristics and feature, but certainly from inaccuracies too. 
\newline

The architecture of the file archive seeks to model an interpretation of a storage area network (SAN) primarily used for research and scientific related big data analysis, operating in a trusted homogeneous computing environment. The illustration at Figure \ref{fig:sofa-overview} depicts the general overview and flow of the system just described, where each component is examined subsequently in Chapter \ref{chp:components}.

\section{Hardware} \label{sec:hardware}
The framework is designed first and foremost to execute on a homogeneous collection of machines connected to a network such as a cluster, but the choice could have been a grid (a collection of heterogeneous machines) too. 
\newline

The implementation will be targeted inexpensive commodity hardware like Dynamo or the Google File System (Section \ref{sec:related}) and will be assembled by JBODs (Defintion \ref{def:jbod}) for this first version.

\vspace*{3mm}
\begin{definition}[JBOD] \label{def:jbod}
\textit{It is an acronym for Just a Bunch of Disks and is a hardware architecture with multiple hard drives, but without configuration of RAID and thus doesn't provide redundancy nor performance improvements.}
\end{definition}

The fact that the underlying component is a file archive related system built by bricks (Definition \ref{def:brick}) make clusters an obvious choice for a prototype.

\vspace*{3mm}
\begin{definition}[Brick] \label{def:brick}
\textit{A brick is defined as a component of a homogeneous distributed system, where each of them is functional equivalent and are contributing uniformly and have equal rights.}
\end{definition}
\vspace*{3mm}

\section{Architectural style} \label{sec:architectural-style}
The overall architecture of the project will be affected by knowledge and benefits from the existing solutions (Section \ref{sec:related}) along with the theoretical background knowledge acquired by Tanenbaum \etal's description of distributed system architectures\cite{Tanenbaum:2006:DSP:1202502}. 
\newline

The result will seek to model the notion of a hybrid related distributed system with a decentralized collection of interconnected peer-to-peer storage nodes accessible by potentially multiple replicated stateless gateway servers and observed by one or more monitors.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.7]{pdf/architecture-overview.pdf}
	\caption[Architectural overview]{Architectural overview of the hybrid related distributed system reflecting a zero-hop distributed hash table ring of storage nodes. \label{fig:architecture-overview}}
\end{figure}

The architecture of the interconnected storage nodes (Figure \ref{fig:architecture-overview}) is highly influenced by Dynamo and will reflect a zero-hop distributed hash table (Definition \ref{def:dht}) and thus provides full data consistency concerning the CAP theorem (Definition \ref{def:cap}). Unfortunately, this also means that complexity regarding the consistency protocol increases at a membership protocol level.
\newline

The design and structure of the storage node will be described as part of Chapter \ref{chp:nodes} in Section \ref{sec:storage}, the gateway in Section \ref{sec:gateway} and the monitor in Section \ref{sec:monitor}.
\newpage

\begin{definition}[Distributed Hash Table] \label{def:dht}
\textit{DHT is undoubtedly the most widely used, organized, peer-to-peer overlay infrastructure, a procedure to arrange processes/nodes and links between them. Each node is assigned a unique identifier and a sub key-space\footnote{The available identifiers between two neighbor nodes}  generated by special hash-function, defined for that particular system and thereby placed onto the ring. Figure \ref{fig:dht} illustrates an example of a DHT ring.}

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.6]{pdf/dht.pdf}
	\vspace*{3mm}
	\caption[]{Example of a distributed hash table example with a key-space size of 100 and 5 storage nodes available. \label{fig:dht}}
\end{figure}
\end{definition}

\section{Partitioning and distribution} \label{sec:pandd}
The partitioning and distribution protocol in \CodeName is based on consistent hashing (Definition \ref{def:ch}) and the round-robin load balancing algorithm (Definition \ref{def:rr}). The downside of this distribution algorithm is that it does not take state, availability and workload among other properties of the servers into account. 
\newline

However, this should be adequate assuming data is large enough and the framework primarily is expected to be operating in homogeneous and high-performance computing environment like a cluster (Section \ref{sec:presumptions}). Furthermore, one of the assumptions (Section \ref{sec:assumption}) is to implement a transparent but suitable load balancing protocol with big data analysis in mind.
\vspace*{3mm}

\begin{definition}[Round-robin] \label{def:rr}
\textit{A simple, fair share and starvation free load balancing algorithm that is easy to implement. The algorithm is distributing elements until there are no more among the available servers in a one-by-one or chunk-by-chunk fashion, thus the name of the algorithm.}
\end{definition}
\vspace*{3mm}

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5]{pdf/rr-partitioning.pdf}
	\caption[Overview of the partitioning and data distribution protocol]{General flow of the partitioning and data distribution protocol in \CodeName, assuming one gateway server \textit{g} and a client \textit{c} attempting to insert data \textit{A} consisting of \textit{n} blocks $\text{d}1 \ldots \text{d}n$.\label{fig:rr-partitioning}}
\end{figure}

Figure \ref{fig:rr-partitioning} illustrates the general flow of the preferred partitioning and distribution protocol. The data is distributed from the client to an arbitrary gateway server, where it is assigned a unique identifier using consistent hashing (as in Amazons Dynamo project) and partitioned into approximately\footnote{As described in section \ref{sec:objectives} is one of the ambitions to store arbitrary sized block.} equal block sizes. The blocks are distributed in a round-robin fashion to the storage nodes, after the metadata for the data is assigned to the root server\footnote{The server responsible for the data, which is determined by the identifier.}.

\section{Naming and virtualization} \label{sec:virtualization}
A requirement is that each instance of \CodeNameFull is associated with a specific global instance name (Section \ref{sec:configuration}) \eg based on the research area or even specific data types. This name is predominantly used to virtualize components, data, and caches and thereby supporting multiple executions of \CodeName instances at once, with own domains.
\newline

Elements in the framework are identified using a structured naming based technique\cite{Tanenbaum:2006:DSP:1202502}, where each semicolon denotes an explicit and more specific context. 

\subsubsection*{Nodes}
The pattern of the naming scheme of node components (depicted at as a general overview at Figure \ref{fig:sofa-overview} and described in Chapter \ref{chp:nodes}) are characterized as following:
\begin{equation*}
	\texttt{sofa}:<\text{instance name}>:<\text{type ref}>:<\text{sequence number}>
\end{equation*}
\texttt{type ref} is one of following component types: gateway, storage or monitor and the \texttt{sequence number} is a positive increasing number in range $0\ldots n$ where $n$ is either the number of gateways, storage nodes or monitors respectively.

\subsubsection*{Data}
\begin{equation*}
	<\text{instance name}>:<\text{data type}>:<\text{data name}>
\end{equation*}

The naming scheme of data (Chapter \ref{chp:components}) are defined as above, where \texttt{instance name} is the same as above, the \texttt{data type} is an optional parameter if the framework supports multiple types. The name of the data is required to be unique and specified as \texttt{data name}. The \texttt{sofa} identifier is not necessary since the data is only accessible in a sofa based context.

\section{Redundancy and replication}
Redundancy in \CodeName is carried out using \eg software RAID-Z in ZFS (Definition \ref{def:zfs}), which provides block-level striping with double distributed parity, that ensures fault tolerance by upto two failed drives. The motivation for this is that the existing replication protocols in \eg Hadoop and Google File System (Section \ref{sec:related} and \ref{sec:replication}) are using a vast amount of space and communication are too expensive considering that the primary usage of this is big datasets.
\newline

If the redundancy protocol however is based on erasure codes then it is no longer replication, but fault tolerance with a degree of durability. This type of responsibility is in this system carried out in hardware by the JBODs, such that there is no need for the extensive amount of network data transfer.

\begin{definition}[ZFS] \label{def:zfs}
\textit{It is composed of one or more virtual storage pools which are a collection of virtual devices that can be interpreted as a RAID (Redundant Array of Independent Disks) set in a standard file system.}
\newline

\textit{ZFS is an extensive 128-bit addressable file system that always is consistent on disk, this is because it performs a copy on write operation when adding new data, \ie writing modified data into a new space and reuse the old space in the future, thus eliminating the write hole error (corrupted file system) \eg on power loss.}
\end{definition}

\section{Recovery} \label{sec:recovery}
The RAID-Z configuration described in the previous section with the purpose of replication are also used as a basic recovery protocol to conceal disk failures. A second recovery initiative is redundant multipaths to efficiently hide server error.
\newline

The system will run on economical low-cost hardware such as JBODs (Definition \ref{def:jbod}) as explained in Section \ref{sec:hardware}. Having SAS-controlled\footnote{Serial Attached SCSI: serial point-to-point protocol for moving data} disks enables fail-over features such as multipathing (Definition \ref{def:multipath}) that allows redundant data paths between host servers and the block-level devices.

\begin{figure}[h!]
	\vspace*{3mm}
	\centering
	\includegraphics[scale=1]{pdf/1host-1disk.pdf}
	\vspace*{3mm}	
	\caption[Simple JBod setup]{JBod setup with one host and one array of disks.\label{fig:1host-1disk}}
\end{figure}
\newpage

Figure \ref{fig:1host-1disk} illustrates a simple example with one host and one array of disks, where:
\begin{itemize}
	\item \textbf{HBA} (Host Bus Adapter) is the local controller on the computer that connects it to other storage or network devices.
	\item \textbf{SIM} (SAS Interface Module) is the storage device controller.
\end{itemize}

Figure \ref{fig:2host-2disk} includes another more complex example to show the scalability with multiple hosts and disks.
\begin{figure}[h!]
	\centering
	\includegraphics[scale=1]{pdf/2host-2disk.pdf}
	\caption[Complex JBod setup]{JBod setup with multiple hosts and multiple disks. \label{fig:2host-2disk}}
\end{figure}

\begin{definition}[DM Multipath] \label{def:multipath}
\textit{Multipathing is a Linux operating system device mapper for SAS-controlled hard drives that provides I/O fail-over and load balancing, by having numerous paths between host and disk. The configuration of DM Multipath is handled at the operating system level and the device controllers on the storage disks is unaware of such configurations.}
\end{definition}

\section{Security} \label{sec:security}
The focus on data integrity and security have predominantly been given low priority, since the system is expected to execute in a trusted environment, exactly like Amazon Dynamo, thus it is intentionally implemented without internal authentication or authorization.
\newline

Nevertheless, security is evidently an influential component and property at the boundary of the system, one of many actions is \eg using the secure HTTPS connection on the network requests to the entry gateway servers instead of ordinary HTTP. Validation of user input is most certainly also a reasonable action when it comes to security.
