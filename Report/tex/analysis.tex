\BigLetter{I}{nteresting} aspects of system design, architecture, and infrastructure from existing big data analysis computational frameworks will be examined in this chapter. Prior architecture presumptions and objectives will additionally be formulated to restrict the scope of the prototype but also to indicate the area of importance. 

Generalized test data within the addressed research areas will subsequently be presented. Those will collectively contribute to a comprehensive investigation challenging various aspects of the project.
\newline

Big data analysis (BDA) is widely used in most research areas, and businesses and the volume, variety, and velocity that data are progressing will continue expanding. The demand for processing large amount data has never been bigger; this includes cracking hidden patterns, unknown correlations and seeking new valuable information.

\section{Observations}
Hadoop (Section \ref{sec:related}), one of the most widely used large-scale frameworks for BDA is characterized in \cite{Mundkur:2011:DCP:2034654.2034670} as \textit{"rather heavyweight and difficult to adapt to custom environments"}. 

Supercomputing enthusiast Glenn Lockwood describes in his blog post \cite{PageLockwoodHadoop} that high-performance computing (HPC) communities keep having troubles understanding the match between Hadoop and HPC. The reason is that it isn't designed with HPC in mind and is developed under other circumstances, and for another purpose than what it is used for nowadays. Additionally, it is written in the programming language Java which core feature is the fact that it is platform-independent \cite{PageJava}, a concept that is completely opposite to HPC where it is favored to optimize code for specific hardware.

\section{Single Master}
A vast majority of the computing platforms discussed in Section \ref{sec:related} are based on a master/slave architecture where the specific master is stateful and could be a potential bottleneck (Definition \ref{def:bottle}) in the system. Additionally is such setting also a single point of failure (SPOF) error, meaning that a fail or crash can be fatal, expensive, or even a catastrophe.
\newline

The single master pattern in Hadoop at the earlier versions (before \textbf{2.x}) was a serious issue because the stateful master that keeps the entire namespace in random access memory (RAM) did not support automatic fail-over. Although the master periodically persists namespace state on disk as a system-record image and moreover keeps a synchronized log of all events too, it is required to replay the record and the logs at a restart to recover the global state.

Later versions support fail-over to a working redundant standby copy, with hardware and communication cost. Other solutions to the SPOF error (Section \ref{sec:related}) such as the Facebook Avatarnode Hadoop extension requires integrations and instances of extra systems like ZooKeeper.
\newline

Systems like the Google File System follows the same stateful metadata master model, whereas DDFS, which is the data access layer of Disco, implements an interesting model where the metadata and data blocks are stored jointly on the cluster hosts. Nevertheless are a simple metadata cache system implemented on the master, which appears like is a single master pattern and is motivated by the preference for consistency over availability in the CAP Theorem (Definition \ref{def:cap}).

\section{Partitioning} \label{sec:partitioning}
The concept of data partitioning has been covered a substantial amount of times by Wilkinson \etal \cite{Wilkinson:1998:PPT:289352} among others and consistent hashing (Definition \ref{def:ch}) is one opportunity to separate data uniformly across multiple machines.

The Amazon Dynamo project (Section \ref{sec:related}) implements consistent hashing with virtual nodes (Definition \ref{def:virtualnodes}) as data partitioning protocol where each data item is identified and determined by a 128-bit identifier generated by an MD5 hash function.
\vspace*{3mm}

\begin{definition}[Virtual nodes] \label{def:virtualnodes}
\textit{Introducing virtual nodes in a consistent hashing ring serves the purpose of loading balancing an non-uniform distribution of existing objects, by replicating points along the circle, likely increasing the total performance by offloading the overloaded potential bottleneck server(s).}
\end{definition}
\vspace*{3mm}

Systems such as Hadoop implements a straightforward and random blocks allocation strategy that strongly seek to distribute new blocks uniformly amongst all the DataNodes. The fundamental consideration behind this logic is trying to avoid situations where newly added nodes become bottlenecks (Definition \ref{def:bottle}) because their overall disk utilization is low. Considerable drawbacks of this strategy are:

\begin{itemize}
	\item Doesnâ€™t consider existing nodes disk utilization (usually forcing multiple and repeatedly load balancing tasks due to an imbalanced cluster).
	\item Doesn't consider the real-time workload on nodes before selecting one.
\end{itemize}

\section{Replication} \label{sec:replication}
The block replica placement strategy in HDFS is a predominant and critical aspect regarding data reliability, performance and potentially network bandwidth utilization too. The number of replicas is user-defined (default is 3) and is placed as following in the system:
\begin{itemize}
	\item First block is placed on the node where the writer is located.
	\item Location of second and third block is a distance based calculation favoring bandwidth and cluster structure.
	\item Rest of the block replicas are placed randomly. 
\end{itemize}

This provides: \textit{"$\ldots$ a tradeoff between minimizing the write cost, and maximizing data reliability, availability and aggregate read bandwidth"} \cite{Shvachko:2010:HDF:1913798.1914427}. The block replica placement strategy can be overridden by a user-specified if necessary but the multiple and repeatedly load balancing tasks are still compelling.

Disco DDFS is using non-RAID (Redundant Array of Independent Disks) disk configuration. However, it provides fault-tolerance by implementing a \textit{N}-way replication\footnote{Amazon Dynamo \cite{DeCandia:2007:DAH:1294261.1294281} implements this protocol too.}, which is similar but simpler version of HDFS.

\section{Presumptions} \label{sec:presumptions}
Based on the analysis and reasoning throughout this chapter are presumptions constructed and has first and foremost the purpose of limiting the prototype and secondly to match the surrounding environment and settings and additionally comply with the general assumption (Section \ref{sec:assumption}).

\begin{itemize}
	\item The solution is operating in a homogeneous and high-performance computing environment like a cluster.
	\item Minimal replication scheme.
	\item Reduced security priority within the system (Section \ref{sec:security}).
\end{itemize}

\section{Objectives} \label{sec:objectives}
\begin{itemize}
	\item Split data such that semantically coherent parts are stored jointly and thus eliminating the data residual problem known from Hadoop.
	\item Store data in arbitrary sized blocks as a consequence of above mentioned.
	\item Eliminating single-point of failure that stems from a stateful master server.
	\item Implement a transparent load balancing protocol with big data analysis in mind.
	\item Design and build a monitor service to measure and observe the system and handle redistribution when necessary.
\end{itemize}

\section{Examples} \label{sec:examples}
The focus of this project will be on three different types of datasets, which combined will contribute to an exploratory research and development process of the big data file archive and will challenge various critical aspects. 

\subsection*{Image data}
The type of image data challenges the aspects of the solution with regards to datasets with fixed sized blocks and thus enabling the opportunity of precisely calculating positions and offsets when fetching the data from disk again. Also does this part cover the problem of choosing, designing and implementing a domain-specific metadata language for describing the content and data context. 

The choice of test image data is limited to high-resolution grayscale X-rays represented as matrices of photon counters.

\subsection*{Text}
Challenges regarding text-based data including first and foremost the semantic correlation, which is well known by nature and furthermore is highly relevant in nowadays prominent companies such as Twitters business model. Secondly, this type of data is typically of a much smaller size than \eg images as described above and thus, demands the support for arbitrary sized blocks on disk. 
\newline

The correlation complications cover additionally the design and implementation of an extension to the chosen metadata context description format, which enables more generic data-specific definitions. The choice of text test data covers \eg readily available raw news articles.

\subsection*{Simulation results}
The third and final type of data is large complex and multidimensional scientific simulation results which easily exceeds petabytes\footnote{1 PB =  1000 terabytes = $10^{15}$ bytes.} of data per dataset. 

The fact that data like this produces single entries in multiple dimensions with a lot of features is a challenge and difficulty because only a subset of those features is interesting from a BDA point-of-view.

The choice of test data for this type is astrophysics and climate modeling data that increases to extremely large collections. Both of these types suffers from the facts mentioned above.