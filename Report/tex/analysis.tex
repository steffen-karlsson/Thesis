\chapter{Analysis}

\BigLetter{I}{nteresting} aspects of system design, architecture, and infrastructure from existing big data analysis computational frameworks will be examined and deliberated throughout this chapter. Prior architecture assumptions and ambitions will additionally be formulated to demarcate partly the scope of the prototype but also to indicate the sphere of importance. Subsequently, will examples of test data be presented, those will collectively contribute to a comprehensive research challenging various aspects of the project.
\newline

BDA is widely used in most research areas, and businesses and the volume, variety, and velocity that data are progressing will continue expanding. The demand for processing huge amount data has never been larger; this includes \eg cracking hidden patterns, unknown correlations and seeking new valuable information in the data.
\newline

\subsection*{Insight}
Hadoop, which is one of the most widely used large-scale frameworks for BDA are as described in \cite{Mundkur:2011:DCP:2034654.2034670} \textit{"rather heavyweight and difficult to adapt to custom environments"}. Supercomputing enthusiast Glenn Lockwood describes in his blog post \cite{PageLockwoodHadoop} that high-performance computing (HPC) communities keep having problems understanding how Hadoop and HPC matches, because the design isn't with HPC in mind and developed under other circumstances, and for another purpose than what it is used for, among other things. 

Additionally, it is written in the programming language Java which core feature is the fact that it is platform-independent \cite{PageJava}, \ie a concept that is diametrically opposite to HPC where it is favored to optimize code for specific hardware.

\subsection*{Single Master}
A vast majority of the computing platforms discussed in section \ref{sec:related} are based on a master/slave architecture where the specific master is stateful and could be a potential bottleneck (defined in Definition \ref{def:bottle}) in the system. Additionally is such a setting also a single point of failure, meaning that a fail or crash can be fatal, expensive, or even a catastrophe.
\newline

Preceding is the case in Hadoop HDFS since it doesn't support automatic failover. The master keeps the entire namespace in RAM (Random Access Memory), which it periodically persist on disk as a system record image, moreover is a log of all events kept synchronized if the master requires a restart.
\newline

Other systems like the Google File System (GFS) follows the same stateful metadata master model, whereas DDFS, which is the data access model of Disco (described in section \ref{sec:related}) implements an interesting and innovative model where the metadata and data chunks are stored jointly on the cluster hosts.
\vspace*{3mm}

\subsection*{Partitioning}
The concept of data partitioning was previously outlined in the technical background in section \ref{sec:partitioning} and as mentioned is consistent hashing one opportunity to separate data uniformly across multiple machines. It is a generalization and foundation of decentralized distributed architectures such as Distributed Hash Tables (DHT). 

The Amazon Dynamo project described in section \ref{sec:related} implements consistent hashing with virtual nodes as data partitioning protocol where each data item is identified and determined by a 128-bit identifier generated by an MD5 hash function.

\section{Assumptions} \label{sec:assumptions}
Following assumptions are constructed first and foremost to limit the prototype to the field of study interesting for this project and secondly because match the surrounding environment and settings.

\vspace*{2mm}
\begin{itemize}
	\item The solution is targeted and used for BDA.
	\item Data entries can be processed and analyzed independently, which can be derived straight forward from the previous statement.
	\item The solution is operating in a homogeneous and high-performance computing environment like a cluster.
	\item The majority of the data is passive, \eg not entirely accessed as much as data are expected to be in GFS.
	\item A whole dataset is accessed, processed or modified at once.
	\item No replication scheme, other than \eg software RAID using ZFS (described in Definition \ref{def:zfs}), due to the expected sizes of the datasets.
	\item Focus on data integrity and security have been given low priority, since the system is expected to execute in a trusted environment, exactly like Amazon Dynamo described in section \ref{sec:related}.
\end{itemize}
\vspace*{3mm}

\begin{definition}[ZFS] \label{def:zfs}
\textit{It is an extensive 128-bit addressable file system that is always consistent on disk, this is because it performs a copy on write operation when adding new data, \ie writing modified data into a new space and reuse the old space in the future, thus eliminating the write hole error (corrupted file system) \eg on power loss.}
\newline

\textit{ZFS is composed of one or more virtual storage pools which are a collection of virtual devices that can be interpreted as a RAID (Redundant Array of Independent Disks) set in a standard file system.}
\end{definition}

\section{Ambitions} \label{sec:ambitions}
Following ambitions for the architectural overview has been established and evaluated based on the assumptions described in the previous section and the experiences taught by studying similar system such as the once outlined in section \ref{sec:related}.

\vspace*{3mm}
\begin{itemize}
	\item Split data such that semantically coherent parts are stored jointly and thus eliminating the data residual problem known from Hadoop.
	\item To store data in arbitrary sized chunks as a consequence of above mentioned.
	\item Eliminating single-point of failure on a stateful master server.
	\item Implement a transparent but suitable load balancing protocol with big data analysis in mind.
	\item Design and built a monitor service to measure and observe the system and handle redistribution when necessary.
\end{itemize}
\vspace*{3mm}

\section{Examples}
The focus of this project will be on three different types of datasets, which combined will contribute to an exploratory research and development process of the big data file archive and will challenge various critical aspects, additionally, will they combined adhere the ambitions described in section \ref{sec:ambitions}. The selected types of data sets are all contemporary and relevant both in the field of study but also in businesses.

\subsection*{Image data}
The type of image data challenges the aspects of the solution with regards to datasets with fixed sized chunks and thus enabling the opportunity of precisely calculating positions and offsets when fetching the data from disk again. Also does this part cover the problem of choosing, designing and implementing a domain-specific metadata language for describing the content and data context. 

The choice of test image data is limited to high-resolution grayscale X-rays represented as matrices of photon counters.

\subsection*{Text}
Challenges regarding text-based data including first and foremost the semantic correlation, which is well known by nature and furthermore is highly relevant in nowadays prominent companies such as Twitters business model. Secondly, this type of data is typically of a much smaller size than \eg images as described above and thus, demands the support for arbitrary sized blocks on disk. 

The correlation complications cover additionally the design and implementation of an extension to the chosen metadata context description format, which enables more generic data-specific definitions. The choice of text test data covers \eg readily available raw news articles.

\subsection*{Simulation results}
The third and final type of data is large complex and multidimensional scientific simulation results which easily exceeds petabytes\footnote{1 PB =  1000 terabytes = $10^{15}$ bytes.} of data per dataset. The fact that data like this produces single entries in multiple dimensions with a lot of features is a challenge and difficulty because only a subset of those features is interesting from a BDA point-of-view.

The choice of test data for this type is astrophysics and climate modeling data that increases to extremely large collections. Both of these types suffers from the facts mentioned above.
